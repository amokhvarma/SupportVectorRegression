{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1701,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "import cvxopt\n",
    "import pandas as pd\n",
    "from cvxopt import matrix,solvers\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1702,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Input\n",
    "df = pd.read_csv('BostonHousing.csv',sep=',',header=None)\n",
    "X = df.values\n",
    "\n",
    "#Removing the First row which contains name of features\n",
    "\n",
    "X = X[1:]\n",
    "X = X.astype(float)\n",
    "\n",
    "# Changing 0-1 binary classification to -1-1 classification so that similarity matrix\n",
    "# is actually indicative of similarity, otherwise (0,0) will give 0\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    if(X[i][3]==0):\n",
    "        X[i][3]=-1\n",
    "\n",
    "y = X[:,-1]\n",
    "X = X[:,:-1]\n",
    "#Normalisation\n",
    "for i in range(X.shape[1]):\n",
    "    X[:,i] = (X[:,i]-np.mean(X[:,i]))/(np.std(X[:,i])+1e-7)\n",
    "\n",
    "mean_y = np.mean(y)\n",
    "std_y = np.std(y)\n",
    "y = (y-mean_y)/(std_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Mean Square Error\n",
    "def pred_loss(y_pred,y):\n",
    "    n=y.shape[0]\n",
    "    c = 1/n * sum(abs(y_pred-y))\n",
    "    return c\n",
    "\n",
    "# Calculate R-2 Score \n",
    "def pred_score(y_pred,y):\n",
    "    m = np.mean(y)\n",
    "    a = sum((y_pred-m)**2)\n",
    "    b = sum((y-m)**2)\n",
    "    return math.sqrt(a/b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-cross validation for Sklearn. This calculates the Score \n",
    "# and Mean Square Error and Prints it as well\n",
    "\n",
    "def kcross_sklearn(X,y,k=5):\n",
    "\n",
    "# groups is a dictionary which contains the data\n",
    "# of each of k pieces mapped to index. \n",
    "# outputs contains the respective outputs\n",
    "    \n",
    "    \n",
    "    score=[]\n",
    "    loss=[]\n",
    "    i=0\n",
    "    n=X.shape[0]\n",
    "    size=n//k\n",
    "    index = 0\n",
    "    groups = {}\n",
    "    outputs = {}\n",
    "    \n",
    "    for i in range(k):\n",
    "        if(index+size>= n):\n",
    "            groups[i] = X[index:n]\n",
    "            outputs[i] = y[index:n]\n",
    "        else:\n",
    "            groups[i] = X[index:index+size]\n",
    "            outputs[i] = y[index:index+size]\n",
    "        index=index+size\n",
    "        \n",
    "    \n",
    "    hold_out = 0\n",
    "# At each stage, one of the k pieces is held out and the model\n",
    "# is trained on the rest. Then it is tested on the held out data\n",
    "    \n",
    "    \n",
    "    for hold_out in range(k):\n",
    "        inp=np.array([])\n",
    "        out = np.array([])\n",
    "        test_data = groups[hold_out]\n",
    "        test_data_output = outputs[hold_out]\n",
    "        \n",
    "        for i in range(k):\n",
    "            if(i==hold_out):\n",
    "                continue\n",
    "            else:\n",
    "               \n",
    "                if(inp.shape[0]==0):\n",
    "                    inp = groups[i]\n",
    "                    out=np.append(out,outputs[i])\n",
    "                else:\n",
    "                    inp = np.append(inp,groups[i],axis=0)\n",
    "                    out = np.append(out,outputs[i])\n",
    "        \n",
    "        \n",
    "#SCI-KIT IMPLEMENTATION :-\n",
    "\n",
    "        svr_lin = SVR(kernel='linear',C=0.1, gamma='auto',epsilon = 0.025)\n",
    "        svr_lin.fit(inp,out)\n",
    "        score.append(pred_score(svr_lin.predict(test_data),test_data_output))  #test_data\n",
    "        \n",
    "        loss.append(pred_loss(svr_lin.predict(test_data),test_data_output))\n",
    "        \n",
    "    print(np.around(score,decimals=4),sum(score)/k)\n",
    "    print(np.around(loss,decimals=4),sum(loss)/k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.883  0.8349 0.6467 0.7697 1.1445] 0.8557480134461715\n",
      "[0.226  0.3596 0.4738 0.5638 0.4105] 0.40676979051488926\n"
     ]
    }
   ],
   "source": [
    "kcross_sklearn(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1706,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kcross_MySVR(X,y,k=5):\n",
    "    score=[]\n",
    "    loss=[]\n",
    "    i=0\n",
    "    n=X.shape[0]\n",
    "    size=n//k\n",
    "    index = 0\n",
    "    groups = {}\n",
    "    outputs = {}\n",
    "    \n",
    "    for i in range(k):\n",
    "        if(index+size>= n):\n",
    "            groups[i] = X[index:n]\n",
    "            outputs[i] = y[index:n]\n",
    "        else:\n",
    "            groups[i] = X[index:index+size]\n",
    "            outputs[i] = y[index:index+size]\n",
    "        index=index+size\n",
    "        \n",
    "    \n",
    "    hold_out = 0\n",
    "    for hold_out in range(k):\n",
    "        inp=np.array([])\n",
    "        out = np.array([])\n",
    "        test_data = groups[hold_out]\n",
    "        test_data_output = outputs[hold_out]\n",
    "        \n",
    "        for i in range(k):\n",
    "            if(i==hold_out):\n",
    "                continue\n",
    "            else:\n",
    "               \n",
    "                if(inp.shape[0]==0):\n",
    "                    inp = groups[i]\n",
    "                    out=np.append(out,outputs[i])\n",
    "                else:\n",
    "                    inp = np.append(inp,groups[i],axis=0)\n",
    "                    out = np.append(out,outputs[i])\n",
    "        \n",
    "        \n",
    "# OWN IMPLEMENTATION :-\n",
    "\n",
    "        svr_lin = SupportVectorRegression(inp,out,'linear', 0.00035,0.025)\n",
    "        svr_lin.fit()\n",
    "        score.append(pred_score(svr_lin.pred(test_data),test_data_output))\n",
    "        loss.append(pred_loss(svr_lin.pred(test_data),test_data_output))\n",
    "        \n",
    "    print(np.around(score,decimals=4),sum(score)/k)\n",
    "    print(np.around(loss,decimals=4),sum(loss)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1707,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Own Implementation\n",
    "\n",
    "class SupportVectorRegression:\n",
    "    #epsilon = 0.1\n",
    "    gamma = 0.1\n",
    "    #c = 0.00035\n",
    "    bias=0\n",
    "    #kernel = 'linear'\n",
    "    \n",
    "# Different Kernel functions can be\n",
    "# specified at the time of initialisation\n",
    "\n",
    "    def kern(self,x1,x2):\n",
    "        \n",
    "        if(self.kernel=='linear'):\n",
    "            #print(1)\n",
    "            return np.dot(np.transpose(x1),x2)\n",
    "        if(self.kernel == 'poly'):\n",
    "            return (np.dot(np.transpose(x1),x2)+1)**2\n",
    "        if(self.kernel == 'rbf'):\n",
    "            #print(np.exp(-self.gamma*np.dot(np.transpose(x1-x2),(x1-x2))))\n",
    "            \n",
    "            return np.exp(-self.gamma*np.dot(np.transpose(x1-x2),(x1-x2)))\n",
    "        else:\n",
    "            print(\"Wrong Kernel\")\n",
    "            return 0\n",
    "    \n",
    "    \n",
    "    def __init__(self,X,y,kernel = 'rbf',c=0.00035,epsilon = 0.25):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.epsilon = epsilon\n",
    "        self.kernel = kernel\n",
    "        self.c = c\n",
    "        self.mean = np.mean(y)\n",
    "        self.std = np.std(y)\n",
    "        self.n = X.shape[0]\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self):\n",
    "        n=self.n\n",
    "        \n",
    "        P = np.zeros([2*n,2*n])\n",
    "# The vector to be optimised is a [2*n,1] vector in which the \n",
    "# first n elements are alpha and next n are alpha*\n",
    "# The formulation can be seen from the report\n",
    "        \n",
    "        for i in range(2*n):\n",
    "            for j in range(2*n):\n",
    "                if((i<n and j<n)):\n",
    "                    P[i][j] = self.kern(X[i],X[j])\n",
    "                if(i>=n and j<n):\n",
    "                    P[i][j] = -1*self.kern(X[i-n],X[j])\n",
    "                if(i<n and j>=n):\n",
    "                    P[i][j] = -1*self.kern(X[i],X[j-n])\n",
    "                else:\n",
    "                    P[i][j] = self.kern(X[i-n],X[j-n])\n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        q = np.zeros([2*n,1])\n",
    "        for i in range(2*n):\n",
    "            if(i<n):\n",
    "                q[i] = self.epsilon-self.y[i]\n",
    "            else:\n",
    "                q[i] = self.epsilon + self.y[i-n]\n",
    "        \n",
    "        \n",
    "        G = np.zeros([4*n,2*n])\n",
    "        G[0:(2*n),:] = np.diag([-1]*(2*n))\n",
    "        G[2*n:,:] = np.diag([1]*(2*n))\n",
    "        \n",
    "        h = np.zeros([4*n,1])\n",
    "        h[0:2*n] = 0\n",
    "        h[2*n:] = self.c\n",
    "        \n",
    "        A = np.zeros([1,2*n])\n",
    "        A[0,0:n] = 1\n",
    "        A[0,n:]=-1\n",
    "                \n",
    "        b = 0\n",
    "                \n",
    "        P_matrix = matrix(P,tc = 'd')\n",
    "        q_matrix = matrix(q,tc = 'd')\n",
    "        G_matrix = matrix(G,tc = 'd')\n",
    "        h_matrix = matrix(h,tc = 'd')\n",
    "        A_matrix = matrix(A,tc = 'd')\n",
    "        b_matrix = matrix(b,tc = 'd')\n",
    "        \n",
    "        \n",
    "        \n",
    "        sol = solvers.qp(P_matrix,q_matrix,G_matrix,h_matrix,A_matrix,b_matrix)\n",
    "        self.alphas = sol['x']\n",
    "        #print(sol['status'])\n",
    "        #print(self.alphas)\n",
    "         \n",
    "# Bias is calculated by using either the inequality or by \n",
    "# averaging over the errors with b=0.\n",
    "    \n",
    "        temp = []\n",
    "        \n",
    "        for i in range(n):\n",
    "            if(self.alphas[i] > 0 and self.alphas[i]<self.c):\n",
    "                temp.append(-(self.mult(self.X[i])+self.y[i]-self.epsilon))\n",
    "        \n",
    "        self.bias = sum(temp)/len(temp)\n",
    "        return \n",
    "    \n",
    "#M = -1*math.inf\n",
    "#       m = math.inf\n",
    "        \n",
    "#         for i in range(n):\n",
    "#             if(self.alphas[i]<self.c or self.alphas[n+i]>0):\n",
    "#                 if(M<(-1*self.epsilon + self.y[i] - self.mult(X[i]))):\n",
    "#                     M = (-1*self.epsilon + self.y[i] - self.mult(X[i]))\n",
    "                    \n",
    "#             if(self.alphas[i+n]<self.c or self.alphas[i]>0):\n",
    "#                 if( m > (-1*self.epsilon + self.y[i] - self.mult(X[i]))) :\n",
    "#                     m = (-1*self.epsilon + self.y[i] - self.mult(X[i]))\n",
    "                    \n",
    "#             if((self.alphas[i]<self.c or self.alphas[n+i]>0) and (self.alphas[i+n]<self.c or self.alphas[i]>0) ):\n",
    "#                 self.bias = (-1*self.epsilon + self.y[i] - self.mult(X[i]))\n",
    "                \n",
    "#         print(M,m)\n",
    "                \n",
    "       # if(self.bias == 0):\n",
    "        #    self.bias = (M+m)/2\n",
    "            \n",
    "        #print(self.X[0:2],self.y[0:5])\n",
    "        \n",
    "        #print(std_y*std_y*pred_loss ((self.pred(X[300:])),y[300:]))\n",
    "        \n",
    "        #print(self.mult(X[100])+self.bias,self.y[100])\n",
    "        \n",
    "        \n",
    "# This multiplies out weight matrix with a \n",
    "# vector X . \n",
    "\n",
    "    def mult(self,X):\n",
    "        n = self.n\n",
    "        alpha = np.zeros([n])\n",
    "        \n",
    "        for i in range(n):\n",
    "            alpha[i] = self.alphas[i] - self.alphas[n+i] \n",
    "        \n",
    "        temp = np.zeros([n])\n",
    "        \n",
    "        for i in range(n):\n",
    "            #print(X,self.X[i],self.kern(X,self.X[i]))\n",
    "            temp[i] = self.kern(X,self.X[i])\n",
    "        #print(temp[0:5],alpha[0:5])\n",
    "        #print(temp@alpha)\n",
    "        \n",
    "        \n",
    "        return temp@alpha\n",
    "\n",
    "#Prediction\n",
    "   \n",
    "    def pred(self,X):\n",
    "        n = X.shape[0]\n",
    "        res = np.zeros([n])\n",
    "        for i in range(n):\n",
    "            res[i] = self.mult(X[i]) + self.bias\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.8475e+02 -8.7209e-01  1e+04  1e+02  7e-14\n",
      " 1: -5.0894e+00 -8.6544e-01  2e+02  1e+00  6e-14\n",
      " 2: -2.5337e-01 -7.9003e-01  6e+00  5e-02  3e-15\n",
      " 3: -5.1297e-02 -6.0868e-01  6e-01  6e-17  2e-15\n",
      " 4: -6.8352e-02 -1.8395e-01  1e-01  3e-17  1e-15\n",
      " 5: -8.7825e-02 -1.1652e-01  3e-02  3e-17  1e-15\n",
      " 6: -9.4126e-02 -1.0681e-01  1e-02  1e-16  7e-16\n",
      " 7: -9.8232e-02 -1.0014e-01  2e-03  1e-16  9e-16\n",
      " 8: -9.8888e-02 -9.9280e-02  4e-04  3e-17  9e-16\n",
      " 9: -9.9038e-02 -9.9095e-02  6e-05  4e-17  1e-15\n",
      "10: -9.9061e-02 -9.9068e-02  7e-06  6e-17  1e-15\n",
      "11: -9.9064e-02 -9.9065e-02  2e-07  1e-17  1e-15\n",
      "12: -9.9065e-02 -9.9065e-02  3e-09  7e-17  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.3169e+02 -8.8117e-01  1e+04  1e+02  6e-14\n",
      " 1: -4.3161e+00 -8.7557e-01  2e+02  1e+00  6e-14\n",
      " 2: -2.4811e-01 -7.9866e-01  7e+00  6e-02  3e-15\n",
      " 3: -4.3286e-02 -6.3536e-01  6e-01  1e-17  2e-15\n",
      " 4: -5.9202e-02 -1.8663e-01  1e-01  1e-16  1e-15\n",
      " 5: -8.0487e-02 -1.0491e-01  2e-02  4e-17  1e-15\n",
      " 6: -8.9258e-02 -9.4250e-02  5e-03  8e-17  1e-15\n",
      " 7: -9.0977e-02 -9.1903e-02  9e-04  1e-16  9e-16\n",
      " 8: -9.1301e-02 -9.1486e-02  2e-04  8e-17  9e-16\n",
      " 9: -9.1381e-02 -9.1393e-02  1e-05  7e-17  1e-15\n",
      "10: -9.1387e-02 -9.1387e-02  3e-07  2e-18  1e-15\n",
      "11: -9.1387e-02 -9.1387e-02  7e-09  7e-17  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.9727e+02 -8.4588e-01  1e+04  1e+02  8e-14\n",
      " 1: -3.4644e+00 -8.4358e-01  1e+02  1e+00  7e-14\n",
      " 2: -1.9283e-01 -7.9856e-01  7e+00  5e-02  4e-15\n",
      " 3: -3.5287e-02 -6.2501e-01  6e-01  6e-17  2e-15\n",
      " 4: -5.1596e-02 -1.4749e-01  1e-01  7e-17  1e-15\n",
      " 5: -7.4526e-02 -9.2042e-02  2e-02  1e-17  1e-15\n",
      " 6: -8.1930e-02 -8.4780e-02  3e-03  3e-17  1e-15\n",
      " 7: -8.2995e-02 -8.3520e-02  5e-04  3e-17  9e-16\n",
      " 8: -8.3222e-02 -8.3266e-02  4e-05  3e-17  1e-15\n",
      " 9: -8.3241e-02 -8.3245e-02  4e-06  3e-17  1e-15\n",
      "10: -8.3243e-02 -8.3243e-02  8e-08  1e-17  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.1605e+02 -9.7579e-01  1e+04  1e+02  8e-14\n",
      " 1: -3.8430e+00 -9.6974e-01  1e+02  1e+00  8e-14\n",
      " 2: -3.2089e-01 -8.3372e-01  8e+00  7e-02  5e-15\n",
      " 3: -4.6624e-02 -6.7014e-01  6e-01  3e-17  3e-15\n",
      " 4: -6.3330e-02 -1.8058e-01  1e-01  1e-17  1e-15\n",
      " 5: -8.4951e-02 -1.0845e-01  2e-02  6e-17  1e-15\n",
      " 6: -9.3446e-02 -9.7532e-02  4e-03  2e-16  1e-15\n",
      " 7: -9.5036e-02 -9.5702e-02  7e-04  2e-16  1e-15\n",
      " 8: -9.5299e-02 -9.5406e-02  1e-04  6e-17  1e-15\n",
      " 9: -9.5338e-02 -9.5362e-02  2e-05  7e-17  9e-16\n",
      "10: -9.5350e-02 -9.5350e-02  4e-07  8e-17  1e-15\n",
      "11: -9.5350e-02 -9.5350e-02  4e-09  1e-16  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.3947e+02 -9.0297e-01  1e+04  1e+02  8e-14\n",
      " 1: -4.1721e+00 -8.9891e-01  1e+02  1e+00  8e-14\n",
      " 2: -2.8383e-01 -8.1218e-01  7e+00  6e-02  4e-15\n",
      " 3: -4.9144e-02 -6.4008e-01  6e-01  3e-17  3e-15\n",
      " 4: -6.5731e-02 -1.8251e-01  1e-01  6e-17  1e-15\n",
      " 5: -8.5038e-02 -1.1263e-01  3e-02  1e-17  1e-15\n",
      " 6: -9.4041e-02 -9.8745e-02  5e-03  1e-16  9e-16\n",
      " 7: -9.5788e-02 -9.6670e-02  9e-04  8e-17  9e-16\n",
      " 8: -9.6151e-02 -9.6242e-02  9e-05  1e-17  1e-15\n",
      " 9: -9.6191e-02 -9.6195e-02  4e-06  1e-17  1e-15\n",
      "10: -9.6193e-02 -9.6193e-02  8e-08  6e-17  1e-15\n",
      "Optimal solution found.\n",
      "[0.6998 0.3846 0.2481 0.5742 0.3554] 0.45242105821265444\n",
      "[0.5361 0.5027 0.7307 0.6104 0.3728] 0.5505349015118421\n"
     ]
    }
   ],
   "source": [
    "kcross_MySVR(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
